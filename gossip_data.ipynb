{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_csv('gossipcop_fake.csv')  #load fake dataset(to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pd.read_csv('gossipcop_real.csv')#load real dataset(to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['id','news_url','tweet_ids'],inplace=True,axis=1)#(to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop(['id','news_url','tweet_ids'],inplace=True,axis=1)#(to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['tag']=0 #fake #(to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['tag']=1  #real(to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teen Mom Star Jenelle Evans' Wedding Dress Is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kylie Jenner refusing to discuss Tyga on Life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quinn Perkins</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I Tried Kim Kardashian's Butt Workout &amp; Am For...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Celine Dion donates concert proceeds to Vegas ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16812</th>\n",
       "      <td>2017 Hollywood Film Awards: The Complete List ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16813</th>\n",
       "      <td>Jada Pinkett Smith explains why son Jaden move...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16814</th>\n",
       "      <td>Tinsley Mortimer Reacts to Luann de Lesseps' R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16815</th>\n",
       "      <td>Prince Harry Carries on Princess Diana’s Legac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16816</th>\n",
       "      <td>Kylie Jenner is actually terrified of butterflies</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16817 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  tag\n",
       "0      Teen Mom Star Jenelle Evans' Wedding Dress Is ...    1\n",
       "1      Kylie Jenner refusing to discuss Tyga on Life ...    1\n",
       "2                                          Quinn Perkins    1\n",
       "3      I Tried Kim Kardashian's Butt Workout & Am For...    1\n",
       "4      Celine Dion donates concert proceeds to Vegas ...    1\n",
       "...                                                  ...  ...\n",
       "16812  2017 Hollywood Film Awards: The Complete List ...    1\n",
       "16813  Jada Pinkett Smith explains why son Jaden move...    1\n",
       "16814  Tinsley Mortimer Reacts to Luann de Lesseps' R...    1\n",
       "16815  Prince Harry Carries on Princess Diana’s Legac...    1\n",
       "16816  Kylie Jenner is actually terrified of butterflies    1\n",
       "\n",
       "[16817 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data1[:4000].append(data2[:12000])    #creating the train data by including 4000 from fake and 12000 from real #(to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Miley Cyrus and Liam Hemsworth secretly ge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paris Jackson &amp; Cara Delevingne Enjoy Night Ou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Celebrities Join Tax March in Protest of Donal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cindy Crawford's daughter Kaia Gerber wears a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full List of 2018 Oscar Nominations – Variety</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>Natalie Maines files for divorce after 17 years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>Janelle Monae's \"PYNK\" Video Teaches an Accura...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>Gwyneth Paltrow and Brad Falchuk Confirm Their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>Alicia Silverstone and Breckin Meyer Are Anyth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>‘CBS Sunday Morning’: Oprah Winfrey Time’s Up ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  tag\n",
       "0      Did Miley Cyrus and Liam Hemsworth secretly ge...    0\n",
       "1      Paris Jackson & Cara Delevingne Enjoy Night Ou...    0\n",
       "2      Celebrities Join Tax March in Protest of Donal...    0\n",
       "3      Cindy Crawford's daughter Kaia Gerber wears a ...    0\n",
       "4          Full List of 2018 Oscar Nominations – Variety    0\n",
       "...                                                  ...  ...\n",
       "11995    Natalie Maines files for divorce after 17 years    1\n",
       "11996  Janelle Monae's \"PYNK\" Video Teaches an Accura...    1\n",
       "11997  Gwyneth Paltrow and Brad Falchuk Confirm Their...    1\n",
       "11998  Alicia Silverstone and Breckin Meyer Are Anyth...    1\n",
       "11999  ‘CBS Sunday Morning’: Oprah Winfrey Time’s Up ...    1\n",
       "\n",
       "[16000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12000\n",
       "0     4000\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['tag'].value_counts()    # consists of 12000 real and 4000 fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=data1[4000:].append(data2[12000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>Robert Pattinson &amp; Kristen Stewart Would ‘Love...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>15 Women Brad Pitt Has Surprisingly Been Linke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>Taylor Swift Wants To Reconcile Her Romance Wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>Khloe Kardashian Has Spent Over $3 Million For...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>Brad Pitt’s Hurricane Katrina homes are fallin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16812</th>\n",
       "      <td>2017 Hollywood Film Awards: The Complete List ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16813</th>\n",
       "      <td>Jada Pinkett Smith explains why son Jaden move...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16814</th>\n",
       "      <td>Tinsley Mortimer Reacts to Luann de Lesseps' R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16815</th>\n",
       "      <td>Prince Harry Carries on Princess Diana’s Legac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16816</th>\n",
       "      <td>Kylie Jenner is actually terrified of butterflies</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  tag\n",
       "4000   Robert Pattinson & Kristen Stewart Would ‘Love...    0\n",
       "4001   15 Women Brad Pitt Has Surprisingly Been Linke...    0\n",
       "4002   Taylor Swift Wants To Reconcile Her Romance Wi...    0\n",
       "4003   Khloe Kardashian Has Spent Over $3 Million For...    0\n",
       "4004   Brad Pitt’s Hurricane Katrina homes are fallin...    0\n",
       "...                                                  ...  ...\n",
       "16812  2017 Hollywood Film Awards: The Complete List ...    1\n",
       "16813  Jada Pinkett Smith explains why son Jaden move...    1\n",
       "16814  Tinsley Mortimer Reacts to Luann de Lesseps' R...    1\n",
       "16815  Prince Harry Carries on Princess Diana’s Legac...    1\n",
       "16816  Kylie Jenner is actually terrified of butterflies    1\n",
       "\n",
       "[6140 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test  #(to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train[\"title\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['len']=data_train['title'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Miley Cyrus and Liam Hemsworth secretly ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paris Jackson &amp; Cara Delevingne Enjoy Night Ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Celebrities Join Tax March in Protest of Donal...</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cindy Crawford's daughter Kaia Gerber wears a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full List of 2018 Oscar Nominations – Variety</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>Natalie Maines files for divorce after 17 years</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>Janelle Monae's \"PYNK\" Video Teaches an Accura...</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>Gwyneth Paltrow and Brad Falchuk Confirm Their...</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>Alicia Silverstone and Breckin Meyer Are Anyth...</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>‘CBS Sunday Morning’: Oprah Winfrey Time’s Up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  tag  len\n",
       "0      Did Miley Cyrus and Liam Hemsworth secretly ge...    0   56\n",
       "1      Paris Jackson & Cara Delevingne Enjoy Night Ou...    0   98\n",
       "2      Celebrities Join Tax March in Protest of Donal...    0   53\n",
       "3      Cindy Crawford's daughter Kaia Gerber wears a ...    0   80\n",
       "4          Full List of 2018 Oscar Nominations – Variety    0   45\n",
       "...                                                  ...  ...  ...\n",
       "11995    Natalie Maines files for divorce after 17 years    1   47\n",
       "11996  Janelle Monae's \"PYNK\" Video Teaches an Accura...    1   73\n",
       "11997  Gwyneth Paltrow and Brad Falchuk Confirm Their...    1   85\n",
       "11998  Alicia Silverstone and Breckin Meyer Are Anyth...    1   90\n",
       "11999  ‘CBS Sunday Morning’: Oprah Winfrey Time’s Up ...    1  103\n",
       "\n",
       "[16000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a235d5fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXwUVbYH8N9JQsIOAQIiiwFEBEG2gKDiBiqgD8VlFB1Fxxl0ns6oM/MUcWZ0XBBRQR0VRcFtBEUYFWTfZJFFguyEJYGQBEI2ICsJWc77o6tDdae6u7qrqrurc76fTz7pvlXdfaur69Ste2/dS8wMIYQQkSUq1BkQQghhPgnuQggRgSS4CyFEBJLgLoQQEUiCuxBCRKCYUGcAANq0acOJiYmhzoYQQtjK9u3b85k5QWtZWAT3xMREJCcnhzobQghhK0R0zNMyqZYRQogIJMFdCCEikAR3IYSIQBLchRAiAklwF0KICOQzuBNRJyJaS0QpRLSPiJ5U0lsR0UoiOqz8j1fSiYjeJaJUItpNRAOs3gghhBCu9JTcqwD8lZl7AhgC4HEi6gVgIoDVzNwdwGrlOQCMAtBd+ZsAYIbpuRZCCOGVz+DOzNnM/KvyuBhACoAOAG4D8Lmy2ucAblce3wbgC3bYAqAlEbU3PedBdCinGL8cPRXqbAgREluPFOBwTnGosxFWth87jZTsojrpldU1mJeciZqa0A+l7tdNTESUCKA/gK0A2jFzNuA4ARBRW2W1DgAyVS/LUtKy3d5rAhwle3Tu3DmArAfPTdPXAwDSp9wS4pwIEXz3zNwCQH7/anfO2ASg7nfy0bo0vLniEKKJcOfAjqHIWi3dDapE1BTAAgBPMXPdU5ZqVY20OqcxZp7JzEnMnJSQoHn3rBBC2Ep+yTkAQOHZyhDnRGdwJ6IGcAT2r5j5v0pyjrO6Rfmfq6RnAeikenlHACfMya4QQgg99PSWIQCzAKQw8zTVooUAxiuPxwP4QZX+oNJrZgiAQmf1jRBCiODQU+d+FYAHAOwhop1K2iQAUwDMI6JHAGQAuFtZtgTAaACpAMoAPGxqjoUQQvjkM7gz80Zo16MDwHCN9RnA4wbzJYQQwgC5Q1UIISKQBHchhIhAEtyFECICSXAXQogIJMFdCCEikAR3IYSIQBLchRAiAklwF0KICCTBXQghIpAEdyGEiEAS3IUQIgJJcBdCiAgUscH9w3VpWLA9K9TZEEKIkPBrmj07mbL0AACEfKorIYQIhYgtuQshRKiQp0HSg0iCuxBCRCAJ7kIIEYH0zKE6m4hyiWivKu0bItqp/KU7p98jokQiOqta9qGVmRdCCKFNT4PqZwDeA/CFM4GZ73E+JqK3ABSq1k9j5n5mZVAIp4W7TmBIl1Zo27xhqLMihFfMoc6BjpI7M68HcEprGRERgN8AmGtyvoRwUVxeiT/P3YEHZv0S6qwIYQtG69yHAchh5sOqtC5EtIOI1hHRME8vJKIJRJRMRMl5eXkGsyEiXU2N43924dnQZkQImzAa3MfBtdSeDaAzM/cH8BcAc4ioudYLmXkmMycxc1JCQoLBbAghhFALOLgTUQyAOwB840xj5gpmLlAebweQBuASo5kUQgjhHyMl9xEADjBz7T3+RJRARNHK464AugM4YiyLQggh/KWnK+RcAJsB9CCiLCJ6RFl0L+o2pF4DYDcR7QIwH8BjzKzZGCuEEMI6PrtCMvM4D+kPaaQtALDAeLaE0BYGPcyEsAW5QzXClZ2rQklFVaizYVwYjNUhhJ1IcI9wA15eid4vLA91NoQQQSbBPcKVV9aEOgtCiBCQ4C6EEBFIgruwlwhsUV2x7yQSJy5Gam5JqLMiIogEd2EL4TD5gVWW7j0JANiddSbEORGRJCKC+4QvkjFi2rpQZyMiDH51Ff727a5QZ0MIYVBEBPcV+3PkktYkucUVmC8TiwthexER3IUQIpyEQzWiBHchhACwM/MMhr62GkXllaHOiikkuAshBIDpKw8hu7Ac24+dDnVWTCHBPcw8M38XVu3P0VyWW1yOcTO34FTpuSDnynzTVhzEf7YcC3U2hIhYEtzDzLzkLPz+i2TNZbM2HsXmIwX4ZltmkHNlvnfXpOLv3+/1vaIQOn24Lg2Ld2eH5LO/33EcszYeDclneyLB3ebySyow6bs9qKiqDnVWhAipKUsP4PE5v4bks5/6Zide/nF/SD7bEwnuNvfq4hTM2ZqBpXtOhjorQogwIsHd5mo4Au/HF0IYJsFd2IqcyvTZeDgfGw7nhTobIoT0TLM3m4hyiWivKu1FIjpORDuVv9GqZc8RUSoRHSSim63KuKhfwuCeEMuwBVdfv521FQ/M+sX09xX6hMMFtZ6S+2cARmqkT2fmfsrfEgAgol5wzK16mfKaD5wTZgshvAuHuxoFIuby0GdwZ+b1APROcn0bgK+ZuYKZjwJIBTDYQP6EEBZatOsEys5FwDSMJoi0k6uROvcniGi3Um0Tr6R1AKDuhJ2lpNVBRBOIKJmIkvPypG5QlwgpUei1Oa0A+SUVoc5GxNqRcRp/mrsDL/ywL9RZCQvhUJVipkCD+wwA3QD0A5AN4C0lXevcp/mVMfNMZk5i5qSEhIQAs1E/hXMJY0fGaZwpM+cO2nEfb8E9H2025b1EXc6J07MLy0Ock/CSXlCK9PzSUGfDsICCOzPnMHM1M9cA+Bjnq16yAHRSrdoRwAljWQxvhWWVSMkuCnU2wsbYDzZh3MdbTXu/tDzXg8yKxkch1P61aD+ue/Mnw++z/dgpbErNN56hAAUU3ImoverpWADOnjQLAdxLRHFE1AVAdwAR3WR/90ebMOqdDSgs828kuTNl55BRUGZRrkLLipMdhfPlihBucorKceeMzbjvk60hK/zp6Qo5F8BmAD2IKIuIHgEwlYj2ENFuANcDeBoAmHkfgHkA9gNYBuBxZo7o++IP5TgmCbn57fV+vW7EtHW45o21VmTJEpmnylBeGdG7st7i+taYYyHnoH7FFVV10oItxtcKzDxOI3mWl/VfBfCqkUxZ7UzZOURHEZo1bFCbdqr0HOJiotAkzudXoulkkX/1lvklge/wYNdMVNcwhk1dixE92+KT8YOC++HCMhTRdw+ExsJd4VMLXS/vUO330kpcMXm1S9qAl1fipun+lb6DLkTHonOIg58OSq8mIeyiXgZ3ACg7V7eK4fiZs15fU1IRfv2BnaV4re1R51dP3s3cvnNVNQG9rrK6RrP6p743pNbUsPRHt5iZzTrqtwrVT7feBffCs94bPquqa1BTo703er+wHL8c1Xs/V12V1YEFvFpefiSTvtvjMj3Yzswz6P3C8trnvV9YjkVeLhm3HClwWd+oEdPWBfS6W97dgEv/scy0fESKlxfvR69/Lpd2D6FbvQvuvvpgX/z8Ukz4crvH5TsyApuC62h+Kbo/vxTf7zge0OvVPJUw1D12dmedqbPc20BSZk8tlnEqsJ5AzgZqd5HcW0ZPwW7B9iwAQEWlwQKC1ufX74siS4TDVxpY62GEW5WiPc2dEQeU7lDL9p7E7f01b9r1KnHiYsN5MHoQV3m4ohHmCHYDZwSfLwXqYcldrxX7tCe/8Bbe+r+0AokTF+PrXzKsyZRB327Pwp6sQiROXIxdmXVL9kaFY714cXklEicuxldbw3++VumSaK5NqflInLgYaXnaV4PurPr5hmq/1rvgrrd0NC85y+/3Pq1Ui3z6c7ru12hVn/jD39LX6gM5yv9cn+uu2p+DxImLUWCg22ao5ShdVGeHeH7LvOIKJE5cjNUaV4VGCtCzNx5F4sTFqNLZnjPqnQ149EvtOXojjbNbopF2Mjurd8E9WH7/ebKug2jxHmMT+m5OK6h9bHbJ47NN6QCA/dmF5r6xAXYt2+494fgOv9hs7hXEG8sPAgDO6QzuKdlFWL7P9QQThhdchszfnoWLJy1BZbVjwyJt+/SS4O4nPT8UBmNVSk6dg8gKucXaoya+qRz0enm7AvjdZ6Ev6dX36mEr4lOkfqeTl6SgqoZRrPQeC0W1iDpOPDDrFywxWIgLREQF9999ts3nOvWlEamo3EefaNWv7701hzF1mX8nAx9vaUub0wpw/Zs/hXd3w3ry+zVDOB3rLywM/rDKERXc13ipRz5+5ixueXdDyMcHr1OKUJ6+8uN+fLQuzfLP12pzeHPFIcs/1w5e/nE/juaXIjVXXwOcsAe9hY5wOhmYod50hfx041HsO1GE73T2M/fUHbKGGX+auwPjh16EpMRWPt+n8GwlnpjzK0b0bOd1vU9MaPDz9/Lz2+TMgPujaykL5xJvqAV4VfPDzuPYfyL8hpReeyAXK1NyMHlsn1BnxSNnQSYcLigDvWPbCNsHd39vKjJ6cp6/PQtH80ux4XAedv7zJs111DfiLNiehQ2H800tDZpVwPi/+btNeicHb1dO/nLvVumsKtEaZiHY3lpxEEO7tsaVF7fx+7X+lg6f/HonAKBZw/A6VB9WqkC1gvuyvSeRcaoUE67pFuxsuXB+1//4fi/2HS/ElDsvD/i9fs04jaV7svH8Lb10DQ7mvp993RlvBdtXy4z9YJPX5duPnUbixMXY56X08+nP2qXmoa+trtMf/GiAM7SYWR9txlvpeY9w6nftb4NURVU1bp6+HlOWGm9LcPfvNam47xPzJiQxQzi1dzz2n+2YvORAyD7fWTBQB9ivt2V6WFufOz7YhI83OOLEn+fuqE1/a4X5vy+z2D64+3LnDEfw33ykwOM6/1q0XzM9u7Act73/s6n5ca/zNvuYzC0ux5Slng8s29Yr+pnxxbuzcTCnuLZ6LVKHL4jQzTJFMO74/feaVA8D3Vn+0T5FfHB3FwbfuWUIhIkL9uBDLw2zVv7o9Nyh+vmm9KA0WAYyUkKpMupidZCGWcgvqcC7qw+75HXO1gwcOKm/jj2YQeRU6Tm8s+qwx4H1tGSeKsPH64/oWreiqhpvLj9oXm+lIJ34wvUEW++Cu5rRhqr1h8JrfPPFe7J1t0Ho+T1aUfJ5YeE+3PbeRtPfV48vN6cju9DzsM7HlGkP9Ta6a5m3LbPO5MqpuSWY/2vdO56fnb8b01YewrZ0xx2UBMKk7/Zg5NsbPL7/zPVpmvW3RgKM3uq3ZxfsxvRVh7DlqOerYHfjZ/+CV5ekILfY92Q2X2w6hvfWpuKjdfpOBkYcKyjFPKWq5viZs/jPlmMmnyhDX4zUM83ebCLKJaK9qrQ3iOgAEe0mou+IqKWSnkhEZ4lop/L3oZWZD4T6GBj9rueDSI8HZ/s/PayV9divLztQOwSCJ/OSM5V8hE6pRqPool0nkGlizx13ecUV+McP+/DQbN/3QlTVeO7Z4Ovq5JkFu/E/bievm99ej8W7HW0G6t9fIFcK769Nwz++3+t7RT38PCE4x5P38vXUUTvdnI5NrKiqdvlvlLfNG/Pez3hmgaNDwQOfbMXfv98bkkZPK+kpuX8GYKRb2koAvZn5cgCHADynWpbGzP2Uv8fMyaZ1Au2iZNe+s74mJPGlpKKqdvhZM/1p7g6MUQXF9YfNnTXeWZVw5qzvcXIO55S4DOugtue476EYit1uIFMHbzNOqsXloQlCVlYBpeWV4OdU/VcEenhrZ1EH8lPKMODhOPCdEXrmUF1PRIluaStUT7cAuMvcbFlnY6pr0Hh/baqln+cssXnjbQLd1NySkB3MWq57Yy3yS86hS0ITDOgcDwAoKKnA7izj48+orzrGB3BV5I0/h+3Wo6cw7uMtSJ9yS51l5RaMp243VhRYhr91fnIXf0MsM+NH1XEWcIh227Di8kqXeZbtxow6998BWKp63oWIdhDROiIa5ulFRDSBiJKJKDkvz7y664yCMuw74TnQpOW51odaPTN5so5JMB7WGDbBWYoYMW1dne6eobwYcE7sXX6uGinZRUjPL8VDn27Dw59tw9kw6IPu5K0UlnmqDHt1lMCNSs8vRUp2cG5ACtdCZyD58vc1m9MK8CdV90Qno8fJ/31r7n0gwWbozggieh5AFYCvlKRsAJ2ZuYCIBgL4noguY+Y6v3BmnglgJgAkJSWZ9tO85o21Zr2VV2bVzzEzjmqMN/39zuMY27+jKZ+h/bk61vFRBhr1jqPNoklsNIDwnsxDHeyHTXX8RrRK5mqpucVoFBuDDi0beV2vqLwSqbkltVcyTte9+VOddbUCjq5vTbXSoZwS5BSVo13zhjjr1rOksroG2/wc4lZvMPU36Lqc2CwskZhWV+62gZmn9bUBaXU8MONK1qiAS+5ENB7ArQDuZ+XIYeYKZi5QHm8HkAbgEjMyahWrS+5OzkC5/dhpXXV7uUWhHQPHJ9Xv2Vm3Gb6hPbB+7iOmrcdVU9b4XO/3nyfjjg82+d2Fz5/eSOrv9viZsxjy2mrN9d5acUj3DVaB9oZSv2qjl7YR58nfX2Z1OtC7y60473i7aTJYAgruRDQSwLMAxjBzmSo9gYiilcddAXQHYH2/JgOOBHjHqb9ylGCdX3JOd1e7YPW3NsvRvOB8l1rWHshFnmr4Y/dg7ukAzjxV5lfJTytg7FFKaTstmN3KqaTCtZHWU/nAynsItIKury6Ovgbqq6quqduv36Sfvd6gHayjLOt0mc85nM3ks1qGiOYCuA5AGyLKAvACHL1j4gCsVA6iLUrPmGsAvEREVQCqATzGzGE9DUpJRfAbK9MLzl/ueRorJa/YcYOLVZwzFHnjrWSnXuYMPLM9DONgBm8H6rK9J/HYfxyTmvuqanE3bOpaXNiiITY9N9zreoVllSg5V4Xjp117G5VXVtdWjdw7c4vPz8vXOasVM4fvXbVesuVv77PXlx2ova3fbHk6RoA9oe495vZ955dUoPBsJVo08t6o6n7i9eTq19eiWcMY7HnxZl3rG6Wnt8w4jeRZHtZdAGCB0UwFU6gbor7amoHmGoNC5ZdU6J77MRDf6ujOqKfboDepucW4uG0zr+tUVdcgJtpYu77R7+lEofcTXXUNo+9LKzSX/eEL/yYy2XO8EIVllYiO9h645/ySgfuvuEj3+1YzI+t0GfI0StLlldUorahC66Zxmq91PwSYGSeLytG+hfe2Bi0vLNTug++poPBrRt2rHbMOSU9dK9VXG1dOWYOWjR3Bu9qtA39OUQWumLwKB14eVZum1ZX4xmnr6qR5UlxehcKySrRobH0vnHp9hyoQ+uDuyfc7fY88Z7W9x43VG2b7CJoAvI6D41RUXolKndPIObm3axjZza8v85zHDR7qnL3Vv/d9aQUGvbIKp2v7V9ddZ+uRU37V4b+25ACufn0tdmk05N07cwsGvrKqTrqzoOreL3/WxqMY+toaHDxZ7JKulU/3NE/fhz98tUn5aifz9nJmxuBXXdsrzihdcLV+7+5dX7XaYAr8bLfzVFAwW70P7uHMU++TZftOBjkndZlVY7D2YK7Pg/nyF1d4HNwtGNYGMJTxDRo9ZdTOVlbjgFvwVFu46wT6v7RS9+etOeB5SkdfbQHu3TW3HHHUpB4r0G5D8VZdZ/UsVnuPF2LAyytdbqQL19qrUKv3wT0kjZY6Lxc8lYJ2aFzK2oFVdwD6M5BVsPiq6nGlnX/3bo6BqPLzigcAopRg6f61cu1/7fwWlVd6bFNQ7/uaGg74t+C8h+XnNP+vEGpqOKAB5eyq3gf3kzoaFkVd4VJYKq+sxlsrI2OaQCtKoBc/v1Qz3dsNZ1HOrq3uAVh5et/H2l0tT3upnhg8+XxVSNdJS7yOy+Qp7u8/UYRnF+zRvb67Hv9Yihun668ftztbB/eiMLot3x9zfskIdRaCJnHiYrz8o/cqFSMFevf6Yk+ccTMn3O8fCBJPPTwKSipqq/3qltzr7qhAd10gdfM7MvWNeOqpa2tlNeNIAN11h031fa9DOLJ1cM+1aanb/dK1SGeACif36Oj25zTLy/ywDOCLzemG82Ol3GKrTwjOkrLFH6ODuptujZ8Z8nVTVOLExVi213d7kfNT7/loM0b40RPFKpmnjA22FyrhNTGjCBvBHP70g588Ty4SDqz+Lpw3+jz1zc6A38PfE4N7Kfz2939Gu+ZxLvOe6gnu/tYkzdVx1er82K1ehlEIxixLdmfrkruwjvuEE4EIxQGYOHGx640pOll5d6kd7Mw8g+X7XHvcuMf2bel1q0X++u2u2seBtBkE2rCqPjlJbxltUnIXmgpKza2KuPzF5ZrVT4HUgfoy6p0NdUrbWiFEvc6crcdMz0cwWRHg9FbLJE5cjJgowtq/XWfK53rqjSOldf9Iyd3GzJqxRospN6OoDtJgtivorUbZnXW+tD4v2fwJSKz24sJ9gb+YfVep+NNtUO+IoIGchP5t4TAckUyCu429vSq8f/SmTQdnkXBowDTis03pfq2vpwpEHXz9bVD112tLU7SHH3D7WK2urlKK902Cu43NCPOGSHXPCzPN2epolMs8VYYJX+ob20Wr65/RXjBbjpg7LZzVzgUwhIPeCdcD4W0i7GluAf3XjNOY9F3dPu6AviEswtG3yZl4b411BTRb17nbveRlRKnOkegC9enP6Za+vxGTvtuD+67oXDvpRqA++MnYFIt6RoEMFn+PhanLD6LIR/VVdQ3wyOf+DYzmi57yttYVyR1us5ExGOeqavD37/dYVoiw2v/Nd8z09MQN3S15f1sH9/rMrqUVs/xrkYH6Zqd6Vjjo8fdltY/nexgVtEB1D4a/k2bsyvLd4+gXP2eJ8mb6qkO2bCsBgB926pvTwQiplrGpL7fYu3eHUWZcWQRrohY7eeGH8+0k/tZrPzGn7jym7kpNmmeXQGFfLenNk18Hfk+DXhLchRC1qt3qd8K12XLJnuxQZyHsSXAXIgJYMUTC8TNlfo9VHixmjJgZLr62aKwpXcGdiGYTUS4R7VWltSKilUR0WPkfr6QTEb1LRKlEtJuIBliScyGE6dQDq72/1r7VHnayMdX4PSVa9JbcPwMw0i1tIoDVzNwdwGrlOQCMgmNi7O4AJgCYYTybQggh/KEruDPzegDuzdy3Afhcefw5gNtV6V+wwxYALYmovRmZdSdjSggh7M6qidCN1Lm3Y+ZsAFD+t1XSOwDIVK2XpaS5IKIJRJRMRMl5eXkBZaA+93MXQkQGq8qoVjSoauW1Thhm5pnMnMTMSQkJCRZkQwgh6i8jwT3HWd2i/HfOIpwFoJNqvY4AThj4HI9+tqghQgghgsWq6mUjwX0hgPHK4/EAflClP6j0mhkCoNBZfWO2HMtnyBFCCGtVVVtTv6y3K+RcAJsB9CCiLCJ6BMAUADcS0WEANyrPAWAJgCMAUgF8DOB/Tc+1EEJEiMUW3ZCla2wZZh7nYdFwjXUZwONGMiWEEMIYW9+hKj0hhRBCm62DuxBCCG22Du7SzV0IIbTZOrgLIYTQJsFdCCEikAR3IYSIQLYO7tJbRgghtNk7uEt0F0IITfYO7lJ2F0IITbYO7v7Ozi6EEPWFrYO7EEIIbRLchRAiAklwF0KICCTBXQghIpCtg7v0lhFCCG22Du7SW0YIIbTZOrgLIYTQpmsmJi1E1APAN6qkrgD+CaAlgD8AyFPSJzHzkoBz6C0PUi0jhBCaAg7uzHwQQD8AIKJoAMcBfAfgYQDTmflNU3LoLQ9SLSOEEJrMqpYZDiCNmY+Z9H66SMldCCG0mRXc7wUwV/X8CSLaTUSziShe6wVENIGIkokoOS8vT2sVn2TgMCGE0GY4uBNRLIAxAL5VkmYA6AZHlU02gLe0XsfMM5k5iZmTEhISAvpslloZIYTQZEbJfRSAX5k5BwCYOYeZq5m5BsDHAAab8BlCCCH8YEZwHwdVlQwRtVctGwtgrwmfoUmqZYQQQlvAvWUAgIgaA7gRwKOq5KlE1A8AA0h3W2YqqZYRQghthoI7M5cBaO2W9oChHPkhNbckWB8lhBC2Yus7VMsqq0OdBSGECEu2Du7nqiS4CyGEFlsH94qqmlBnQQghwpKtg7sQQghtEtyFECIC2Tq4Szd3IYTQZu/gLncxCSGEJlsHdyGEENokuAshRASydXCXShkhhNBm6+AuhBBCmwR3IYSIQLYO7jIopBBCaLN1cBdCCKHN1sG9cWx0qLMghBBhydbBvVtC01BnQQghwpKtg7sQQghthmZiAgAiSgdQDKAaQBUzJxFRKwDfAEiEY6q93zDzaaOfJYQQQh+zSu7XM3M/Zk5Snk8EsJqZuwNYrTwXQggRJFZVy9wG4HPl8ecAbrfoc4QQQmgwI7gzgBVEtJ2IJihp7Zg5GwCU/23dX0REE4gomYiS8/LyTMiGEEIIJ8N17gCuYuYTRNQWwEoiOqDnRcw8E8BMAEhKSgrofiRmuY1JCCG0GC65M/MJ5X8ugO8ADAaQQ0TtAUD5n2v0c7TIeO5CCKHNUHAnoiZE1Mz5GMBNAPYCWAhgvLLaeAA/GPkcIYQQ/jFaLdMOwHdKCToGwBxmXkZE2wDMI6JHAGQAuNvg5wghREQa1r2NJe9rKLgz8xEAfTXSCwAMN/LeQghRHzSItqbTotyhKoQQIWRVxxAJ7kIIEYEkuAshRAhZ1etPgrsQQkQgWwd36eYuhBDabB3chRBCaLN1cCdI0V0IIbTYOrhb1D1UCCFsz9bhMTrK1tkXOv3lxktCnQURJppE4LzJVtU/2Do6tmseF+osiCCQyjfh1K1t5M2bbNXYtrYO7nLQ1w9RUbKnhYOM8q2frYP7BS0ahToLIghuvuyCUGdBhAnp/qyfrYO77Gh7GNP3QkOvbxAtO1o4SMldP1sHd9nR9jBx1KWGXi/7WUQyaVAVtiWxWZhFrtb1s3Vwlx1dP8h+FsJ/tg7uQgghtAUc3ImoExGtJaIUItpHRE8q6S8S0XEi2qn8jTYvu66kLlYIIbQZmWavCsBfmflXZZLs7US0Ulk2nZnfNJ49IYQ4Twp0+gUc3Jk5G0C28riYiFIAdDArY0II4U7aX/Qzpc6diBIB9AewVUl6goh2E9FsIor38JoJRJRMRMl5eXkBfm5ALxNBZnSOyNZNZZiJcJc+5RakT7kl1NkQKoaDOxE1BbAAwFPMXARgBoBuAPrBUbJ/S+t1zDyTmZOYOSkhISGgz5ZLtPqhaZyR2kMRat0jcDwYOzAU3ImoAUkhi4kAABIZSURBVByB/Stm/i8AMHMOM1czcw2AjwEMNp5NEUpXX9wm1Fmo47oegRUIROS5d1CnUGfBkAYWjV1upLcMAZgFIIWZp6nS26tWGwtgb+DZE+FgeM+2hl4vV1j+ufYSOXH5Y/LYPjj86ijT37dDy+CMXRVj0fAaRk4ZVwF4AMANbt0epxLRHiLaDeB6AE+bkVEROg9dmehznYRmxurFfV0dxLiNDGnX5pYWjRqgTdNYr+t8Mj6pTtqHvx1oVZYsZ3XbWFQUWVL6XfO3azXT+3Ro4fV1/x7X3/S8BCLgb4SZNzIzMfPlzNxP+VvCzA8wcx8lfYzSq0bYGBFh1ws3eV0n1svBpefgvvmydj7y4Ps97KBJbDSm39PP6zoNoqPqlBpjY+z1BVzQvGHtYzOv3IL5LcTFaE8M0qyh9zagpj6WB4vcoSp0adGogV/r/49qJEhmoGtCE6/rPzA0MZBs1WoThB41U++63PB7EOkrZS5/+hr88vzw86+z2bXKc6O1B4u774rO6NRKf3VH1zbefzeRgCwqudSL4B4bUy82M6w1a+j75ODPQa/WsnGDoJTsm+vYBn95GjGzaVwMEgI4Yd01sKPRLHk0yUPA1hKl2iHqfXNRq8Z+nagax7mWnsNhJia7tCHVi6gXJzNp24K3g0a9bMI1Xa3PjAYrTiDqUvygxHhseW649oo6P/vCFg19r+QH9TYPvEjzlhVL1dS4Pr//ios8rjuqt7FJXVb9RbuO3WrdfFzVBkqing5GGwvtZtzgzn6tv+Lpa0z5XL0lotZN6jZIBqPSwuoSW7vmDXGBzuDcsEFwD93PHh6EgRe10r2+pxNhu+b+nXxqVF/6jn/c6PUE28Tg/RAXq64KnPdWaH2emb1bhnVvgz/d0N2091OT4C7q8Pdkdkm7ZoZKtWv/dl3gL/ZDy8bmV6voFa98tq+2B70aRHk+dBNbNzblM9SGdG0d8GvV1TC39fNvVi51cI/XOKmbpXMrx3fmrNYa298xksqmiTdg0RNXu6zr66Y6f6rvhnRtjWiL5giuH8Hd4HcXTk1ZcUFoP4i3MAj+YViXOmkXKQdWh/jA6tw7xesLZt66qP3vdd1cnjsPdn94O0iv7p6ATx8ahPfuG+CS3lzVs8LbhYHu3yARbrjU0fPopl7tQtogeYFGKd3fxsMaty/F/dXvu32fgXL2gOl9YXPH5ygf1L5FI/Tp6Nr10dcV3MCL4vGGqvHdeaIINlsH98ax2l2VzObt97j0yWEBv+9793nvD3vvoE643u1OzK2ThuPPN1zs872bGbhEfTCAnivqy+37r3Ct1lFfxo7u0x6t3EpgUUpQ/Oi3A/HB/doHa/uWroHCOd7MQ1cm4tOHB+nK47DuCZh+T1/NZU/feAneuOty/PS36/DQlYmY/9hQzfX6dWpZ+/iZkT1clnmqkx552QV47Y4+uP7StnV6HQ3veb4LaBM/fs8tdJyAB3dphXfuNdbnur2f1ShOBML3j1+FWeOT0La55yvB5j66Ddb4iKTeenHN/cMQ75kE8PJtlwGo29vKaIHuZlX9/1Vu93A8NcKaahh3tg7uw7q3qRN4tXaonl4HPdo187jMW0+Pnu2b1z5u3jAGq/6iv/751su9X6JOufPyOncrtmwci7/c1MPDK86LNlAv6KkEelMvz33RP3pgIN66uy9eub03nr+lZ+1Be/Nl7dC+RSO0VA7CBtFRLgFSLb5JLEb3aa+5bN6jQ2vHKGkcG40Xx1yGV8f2xgv/0wttmsbV+R2oS0sz7h+Ajx9MUtI74stHBmPab1yDfIPoKNyd1AmJbZrgxTGXoa2HoDa4y/l659ZNYutcSfVS/R6cbut3oculvLr3VjQRBnR2fB9j+rqW8NSlXPceX0+PuEQzf+4niDg/6ua1CjFfTxiKd8f1R8MG2iee3yR1xJw/XKG57IIWDTG8Zzu86+UE0zjWNbiPvMy1UbTGvejuRuun+uzIS/HW3X0xtJvvqqQHhiZi6l2X420f9x44zRqfhOaNzufZUxWNurdQI7fvblj34AznER697QNERLj/is74z5YMPHZtN3Rp0xhDu7XG0ieHYcW+HFzctilOFpXjroEd0fdfK2pfd/8VndHrwubo0LIRdmScwTurD6Nt8zgczCnW/Jx/3toLD87+Bf06tUTSRfH4ZONRAHXriu8Y0BEXt22Gvp1aYlfmGQDAn264GP9ek4pBifHYln4agOMHcrayGgDw9j398NQ3O035Pv55ay/ccnl7rE7JxYJfs7D92OnaZW/f0w9tm8dhT1Yh+nZqiR0ZZ7DmQA4mjroUd87YrOv9h3ZrjRX7czSXtWkahztV3fAmje6JxNZN8MjVjmqY6ff0w8Kdx3HZhc19Nnwtf+oa3Pz2epe09i0a4cc/X43Pfk7HuMGdERMd5bXnxPR7+uHvt/TEj7uzMbL3BS6Bclh3xwlz8pIDyC+p8L7RXjSIjsLGZ2/AoFdX1aYN7tIK+7OLXNaLceut1b9TSzx6TVfEN4lFi8YNam+W0Qquu/55Exb8moVBiY6TSqMG0Xj6xu4Y278D/vrtLpd1H722Kx6+qgveWX0IgGsVHpFrdcLY/h0QE0X4dnsWAGDab/piQOd4XPfmT7XrTL3rcnRu3RidNerwP/ztQHSMb4TeHu7WbKAqXMQ3icWnDw2qbfNQ3wTUrGEMThY57otYtOsE4hpEYdb4JJRX1uDxOb/6/K2oRwx1Xsm3b9EQt3uoCmnUILr22HP6TdL5sWnilEDs6WQ2vGc7XNG1NWJjojCgczwKSs7h1SUptcs/U64i1UF/VO8LMGn0pYhvHIu+nVqie9umiG/cAKfLKi2tZrV1cAfOdyXr3Kox7hnkqA7o2b65S4kaAOY/NhTHz5zFkbxS/PG6brU7z/l6rS958tg+GNwlHqUV1bXrXKT80Cdc0xVd3OoznaWrwYnxtcH94au6IEo5CQ2evBrtldKM0+39O+B02Tm8vzYV+SXnMGn0pdhy5BRGKOs4A8O1lyS4dPX68w0X4901qQAcJdP4JrG1jV73XdEZw3u2xfc7jmN0n/ZYuT+n9sd+ZTdHqWFI19b4o1LP3LpJLApKz+Gde/uhbTNHiXXDM9dj2NS1Lrf933/FRfjXov2edoWLJnEx+IOqy2KrJrF46CpHoH/ltt5YtOsEAEcgd3exh77McTHRePTabprLnKWj1+7oU1sH37ppHMZ7GTph/mNDXYKZuwV/HIoNh/Px9qrDABxXNM6g1bxhDMb0vRAx0VG4o38H/HfHccTFROFvN/dAy8YNsO5QHnZknMHIyy7A8Etdx+YhIjw3umftc2fPF63g3qJxA/xOOUG+fNtluLp7Qp3fndNzoxzv+cT13RETFYV7B3fGsYJSAEDvC1tgz/HC2nVv7NUOo/u0rw3udwxwnJjfvLsvMgpKcXX3BJerFHcjfXQ7HNHT9SrvetV38Mn4JLy3JhU9LmiGET3bYcmebLRr3hCLdp1AbHRU7fGRXdgTo/u0R0p2Ue3x6ryq7NKmCe4Z1AmXtDv/W3lm5KVo1SQWt15+/urvldt7Y8X+HKw/5BhWfEzfC7F4TzZKKqo0833XwI44WViOR6917W779YQhKDxbCcARuF+5vQ8AoKKqGiUVVejdoQUaRBOu63F+Oz9+MAlN4qIRFUWYcI3r73bzc8Mx46e0gKpAdWPmkP8NHDiQA1V09hxPXryfKyqrA3p9dXUNT12WwnnF5bxo13F+Z9UhfvrrHfzNtozadWpqavitFQc5+8xZLq+s4smL93NxeWXt8nnbMviWd9fXphWXV/LvP9/G76w65PJZ7689zEfySvzKX0VldZ3PY2YuKa/kF37Yy/nF5f5uch2pucU846fUOukz16XxoZNFLmmf/XyUf9h5nD/8KZW3HingearvyR/ztmXwlrR8j8tnrkvj15akeF1H7Vh+Kb+35jDX1NT4lY+fDubyol3Hva5zqqSCpyxN4arqGi6tqOTJi/fz2XNVtcurq2v4zeUHOKfwbG3aiTNl/NaKg7ryk1tUzm8sO8DV1f7lfXNaPs9PzuT//prJP6fmaa5TU1PDb688xFmny/jLzek8ZWkKD5m8iiurHMfLF5vTeVfmad2fuTPjNH+5Od3j8mV7s3nlvpN+bQczc2VVNb+2JIXPlJ7zup76WNSr6Ow5fnnRPn5p0T4+U3qOD+cU8Zh/b+B1B3P9zme4AZDMHuIqcRjcbpWUlMTJycmhzoYQQtgKEW1n5rojzcHmDapCCCG0SXAXQogIJMFdCCEikAR3IYSIQBLchRAiAklwF0KICCTBXQghIpAEdyGEiEBhcRMTEeUBOGbgLdoAyDcpO6EUKdsByLaEo0jZDkC2xekiZk7QWhAWwd0oIkr2dJeWnUTKdgCyLeEoUrYDkG3RQ6plhBAiAklwF0KICBQpwX1mqDNgkkjZDkC2JRxFynYAsi0+RUSduxBCCFeRUnIXQgihIsFdCCEikK2DOxGNJKKDRJRKRBNDnR93RNSJiNYSUQoR7SOiJ5X0VkS0kogOK//jlXQioneV7dlNRANU7zVeWf8wEY0P4TZFE9EOIvpRed6FiLYq+fqGiGKV9DjleaqyPFH1Hs8p6QeJ6OYQbUdLIppPRAeU/TPUrvuFiJ5Wfl97iWguETW0y34hotlElEtEe1Vppu0HIhpIRHuU17xLpDWZoWXb8Yby+9pNRN8RUUvVMs3v2lNM87Q/vfI0RVO4/wGIBpAGoCuAWAC7APQKdb7c8tgewADlcTMAhwD0AjAVwEQlfSKA15XHowEsBUAAhgDYqqS3AnBE+R+vPI4P0Tb9BcAcAD8qz+cBuFd5/CGAPyqP/xfAh8rjewF8ozzupeyrOABdlH0YHYLt+BzA75XHsQBa2nG/AOgA4CiARqr98ZBd9guAawAMALBXlWbafgDwC4ChymuWAhgVxO24CUCM8vh11XZoftfwEtM87U+veQrmD9HkL3MogOWq588BeC7U+fKR5x8A3AjgIID2Slp7AAeVxx8BGKda/6CyfByAj1TpLusFMf8dAawGcAOAH5UDJl/1A67dJwCWAxiqPI5R1iP3/aReL4jb0RyOgEhu6bbbL3AE90wlsMUo++VmO+0XAIluQdGU/aAsO6BKd1nP6u1wWzYWwFfKY83vGh5imrfjzNufnatlnD9qpywlLSwpl7/9AWwF0I6ZswFA+e+cMt3TNoXLtr4N4BkANcrz1gDOMLNzKnl1vmrzrCwvVNYPh23pCiAPwKdKFdMnRNQENtwvzHwcwJsAMgBkw/E9b4c994uTWfuhg/LYPT0UfgfHlQPg/3Z4O848snNw16o7C8t+nUTUFMACAE8xc5G3VTXS2Et60BDRrQBymXm7OlljVfaxLOTbAkeJdQCAGczcH0ApHJf/noTttij10bfBcXl/IYAmAEZ5yVfYbosO/uY9LLaJiJ4HUAXgK2eSxmqmb4edg3sWgE6q5x0BnAhRXjwiogZwBPavmPm/SnIOEbVXlrcHkKuke9qmcNjWqwCMIaJ0AF/DUTXzNoCWRBSjka/aPCvLWwA4hfDYliwAWcy8VXk+H45gb8f9MgLAUWbOY+ZKAP8FcCXsuV+czNoPWcpj9/SgURp3bwVwPyt1KvB/O/LheX96ZOfgvg1Ad6UVORaOxqGFIc6TC6VlfhaAFGaeplq0EICzRX88HHXxzvQHlV4BQwAUKpelywHcRETxSkntJiUtaJj5OWbuyMyJcHzXa5j5fgBrAdzlYVuc23iXsj4r6fcqvTa6AOgOR6NX0DDzSQCZRNRDSRoOYD9suF/gqI4ZQkSNld+bc1tst19UTNkPyrJiIhqifDcPqt7LckQ0EsCzAMYwc5lqkafvWjOmKfvH0/70LBgNJhY2YIyGowdKGoDnQ50fjfxdDcfl024AO5W/0XDUoa0GcFj530pZnwC8r2zPHgBJqvf6HYBU5e/hEG/XdTjfW6ar8sNMBfAtgDglvaHyPFVZ3lX1+ueVbTwIi3ov6NiGfgCSlX3zPRy9LGy5XwD8C8ABAHsBfAlHLwxb7BcAc+FoK6iEo+T6iJn7AUCS8r2kAXgPbo3oFm9HKhx16M5j/0Nf3zU8xDRP+9Pbnww/IIQQEcjO1TJCCCE8kOAuhBARSIK7EEJEIAnuQggRgSS4CyFEBJLgLoQQEUiCuxBCRKD/B50n/6PFHm8QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train.len.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def clean_data(text):  #function to remove punctuatuions, tokenize the text and lower the text and remove stopwords\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)  #no pinctations\n",
    "    tokens = word_tokenize(text_nopunct)                       #convert into tokens\n",
    "    lower_tokens=[x.lower() for x in tokens]                     #lowercase the tokens\n",
    "    stoplist = stopwords.words('english')\n",
    "    tokens_without_sw=[word for word in lower_tokens if not word in stopwords.words()]\n",
    "    text_final=' '.join(tokens_without_sw) \n",
    "    return text_final\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['text_Final'] = data_train['title'].apply(lambda x: clean_data(x)) #claeaning the trAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Text_Final']=data_test['title'].apply(lambda x: clean_data(x))#claeaning the TESTING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokens(text):\n",
    "    return word_tokenize(text)\n",
    "#TOKENIZING THE CLEANED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['tokens']=data_train['text_Final'].apply(lambda x: make_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['tokens']=data_test['Text_Final'].apply(lambda x: make_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=data_train['tag'].values    #GETTING THE RESULT VALUES TO TRAIN ON\n",
    "y_test=data_test['tag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c43414c436d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_training_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_sentence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTRAINING_VOCAB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_training_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s words total, with a vocabulary size of %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_training_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_VOCAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max sentence length is %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentence_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3959375"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(training_sentence_lengths)/16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAINING_VOCAB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c102c34d889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_VOCAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_Final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtraining_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_Final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAINING_VOCAB' is not defined"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH=20   #number of words in a paragraph  #(to change) according to question\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train['text_Final'].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train['text_Final'].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "train_cnn_data = pad_sequences(training_sequences, \n",
    "                               maxlen=MAX_SEQUENCE_LENGTH)##trining data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51628 words total, with a vocabulary size of 10114\n",
      "Max sentence length is 24\n"
     ]
    }
   ],
   "source": [
    "all_testing_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "testing_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TESTING_VOCAB = sorted(list(set(all_testing_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_testing_words), len(TESTING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(testing_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH=20\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)#test_data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15763, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
    " EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    " train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM,SpatialDropout1D,Reshape\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.layers import Conv2D, GlobalMaxPooling2D,MaxPool2D,MaxPool3D,GlobalAveragePooling2D,Conv3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,4]\n",
    "num_filters = 32\n",
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "maxlen=MAX_SEQUENCE_LENGTH  #takin max length of para as 20 and then converintg to 5 cross 4\n",
    "def get_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((5, 4, 300))(x)\n",
    "    print(x)\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2), activation='relu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 2), activation='relu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], 2), activation='relu')(x)\n",
    "    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], 2), activation='relu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool2D()(conv_0)\n",
    "    maxpool_1 = MaxPool2D()(conv_1)\n",
    "    maxpool_2 = MaxPool2D()(conv_2)\n",
    "    maxpool_3 = MaxPool2D()(conv_3)\n",
    "        \n",
    "    z = concatenate([maxpool_0, maxpool_1, maxpool_2, maxpool_3],axis=1)\n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "        \n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"reshape_18/Identity:0\", shape=(None, 5, 4, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 20, 300)      4728900     input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_18 (SpatialDr (None, 20, 300)      0           embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 5, 4, 300)    0           spatial_dropout1d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 5, 3, 32)     19232       reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 3, 32)     38432       reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 3, 3, 32)     57632       reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 2, 3, 32)     76832       reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_68 (MaxPooling2D) (None, 2, 1, 32)     0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_69 (MaxPooling2D) (None, 2, 1, 32)     0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling2D) (None, 1, 1, 32)     0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling2D) (None, 1, 1, 32)     0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 6, 1, 32)     0           max_pooling2d_68[0][0]           \n",
      "                                                                 max_pooling2d_69[0][0]           \n",
      "                                                                 max_pooling2d_70[0][0]           \n",
      "                                                                 max_pooling2d_71[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 192)          0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 192)          0           flatten_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            193         dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,921,221\n",
      "Trainable params: 4,921,221\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 1600 samples\n",
      "Epoch 1/10\n",
      "14400/14400 [==============================] - 41s 3ms/sample - loss: 0.5156 - accuracy: 0.7550 - val_loss: 0.2641 - val_accuracy: 0.9312\n",
      "Epoch 2/10\n",
      "14400/14400 [==============================] - 41s 3ms/sample - loss: 0.3897 - accuracy: 0.8260 - val_loss: 0.2762 - val_accuracy: 0.9056\n",
      "Epoch 3/10\n",
      "14400/14400 [==============================] - 40s 3ms/sample - loss: 0.3024 - accuracy: 0.8702 - val_loss: 0.2660 - val_accuracy: 0.8925\n",
      "Epoch 4/10\n",
      "14400/14400 [==============================] - 40s 3ms/sample - loss: 0.2316 - accuracy: 0.9053 - val_loss: 0.2844 - val_accuracy: 0.8831\n",
      "Epoch 5/10\n",
      "14400/14400 [==============================] - 40s 3ms/sample - loss: 0.1658 - accuracy: 0.9332 - val_loss: 0.3848 - val_accuracy: 0.8475\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "hist = model.fit(train_cnn_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test_cnn_data)    \n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "for i in range(len(y_test)):   #creaing the predicted class array\n",
    "    if(y_test[i][0]<0.5):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_metric(output_class_pred,original_ans):\n",
    "    rightly_predicted=0\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        if(original_ans[i]==output_class_pred[i]):\n",
    "            rightly_predicted+=1\n",
    "        \n",
    "        \n",
    "    print(\"Overall_acuracy:\",rightly_predicted/len(output_class_pred))\n",
    "    print('TP',TP)\n",
    "    accuracy=rightly_predicted/len(y_test)\n",
    "    print(classification_report(original_ans,output_class_pred))\n",
    "    print(confusion_matrix(original_ans,output_class_pred))\n",
    "    TN=confusion_matrix(original_ans,output_class_pred)[0][0]\n",
    "    TP=confusion_matrix(original_ans,output_class_pred)[1][1]\n",
    "    FP=confusion_matrix(original_ans,output_class_pred)[0][1]\n",
    "    FN=confusion_matrix(original_ans,output_class_pred)[1][0]\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recalll=TP/(FN+TP)\n",
    "    F1=2*precision*recalll/(precision+recalll)\n",
    "    sensiti=TP/(TP+FN)\n",
    "    specifici=TN/(TN+FP)\n",
    "    numerator=TP*TN - FP*FN\n",
    "    \n",
    "    denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
    "    MCc=numerator/denominator\n",
    "    G_mean1=np.sqrt(sensiti*precision)\n",
    "    G_mean2=np.sqrt(sensiti*specifici)\n",
    "    print('precision:' ,TP/(TP+FP))\n",
    "    print('recall:',TP/(FN+TP))\n",
    "    print(\"F1:\",F1)\n",
    "    print(\"Specificity:\",TN/(TN+FP))\n",
    "    print(\"Sensitivity \",TP/(TP+FN))\n",
    "    print('G-mean1:',np.sqrt(sensiti*precision))\n",
    "    print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
    "    print(\"MCC :\",MCc)\n",
    "    acc=[]\n",
    "    pre=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    specificity=[]\n",
    "    sensitivity=[]\n",
    "    GMean1=[]\n",
    "    Gmean2=[]\n",
    "    MCC=[]\n",
    "    tp=[]\n",
    "    fp=[]\n",
    "    fn=[]\n",
    "    tn=[]\n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    recall.append(recalll)\n",
    "    f1.append(F1)\n",
    "    specificity.append(specifici)\n",
    "    sensitivity.append(sensiti)\n",
    "    GMean1.append(G_mean1)\n",
    "    Gmean2.append(G_mean2)\n",
    "    MCC.append(MCc)\n",
    "    tp.append(TP)\n",
    "    fp.append(FP)\n",
    "    tn.append(TN)\n",
    "    fn.append(FN)\n",
    "    data={'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n",
    "    metric=pd.DataFrame(data)\n",
    "    return metric\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall_acuracy: 0.8009771986970684\n",
      "TP 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.62      0.57      1323\n",
      "           1       0.89      0.85      0.87      4817\n",
      "\n",
      "    accuracy                           0.80      6140\n",
      "   macro avg       0.71      0.73      0.72      6140\n",
      "weighted avg       0.81      0.80      0.81      6140\n",
      "\n",
      "[[ 818  505]\n",
      " [ 717 4100]]\n",
      "precision: 0.8903365906623235\n",
      "recall: 0.8511521694000416\n",
      "F1: 0.8703035448949269\n",
      "Specificity: 0.618291761148904\n",
      "Sensitivity  0.8511521694000416\n",
      "G-mean1: 0.8705239345580761\n",
      "G-mean2 0.7254380565038355\n",
      "MCC : 0.4457413935598297\n"
     ]
    }
   ],
   "source": [
    "resi=check_metric(output_class_pred,original_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE BELOW CODE WILL PREPARE A RESULTS.CSV FILE IN YOUR DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "resi.to_csv('results.csv', mode='w', index = False, header=resi.columns,columns=resi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('GOSSIP_mod1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 2.   CAN BE CHANGED BY CHANGING THE FILTER SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,4]\n",
    "num_filters = 32\n",
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "maxlen=MAX_SEQUENCE_LENGTH\n",
    "def get_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((5, 4, 300))(x)\n",
    "    #print(x)\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2), \n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 2),\n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], 2), \n",
    "                                                                                    activation='relu')(x)\n",
    "   \n",
    "    \n",
    "    \n",
    "    conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 1), \n",
    "                                                                                    activation='relu')(x)\n",
    "    conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2), \n",
    "activation='elu')(x)\n",
    "    conv_6 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 3), activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    maxpool_0 = MaxPool2D()(conv_0)\n",
    "    maxpool_1 = MaxPool2D()(conv_1)\n",
    "    maxpool_2 = MaxPool2D()(conv_2)\n",
    "    #maxpool_3 = MaxPool2D()(conv_3)\n",
    "    \n",
    "    maxpool_4 = MaxPool2D()(conv_4)\n",
    "    maxpool_4=Flatten()(maxpool_4)\n",
    "    maxpool_5 = MaxPool2D()(conv_5)\n",
    "    maxpool_5=Flatten()(maxpool_5)\n",
    "    maxpool_6 = MaxPool2D()(conv_6)\n",
    "    maxpool_6=Flatten()(maxpool_6)\n",
    "    #maxpool_7 = MaxPool2D()(conv_7)\n",
    "   # maxpool_7=Flatten()(maxpool_7)\n",
    "        \n",
    "    w=concatenate([maxpool_4, maxpool_5, maxpool_6],axis=1)    \n",
    "    z = concatenate([maxpool_0, maxpool_1, maxpool_2],axis=1)\n",
    "    \n",
    "    z = Flatten()(z)\n",
    "    z=concatenate([w,z],axis=1)\n",
    "    z=Dense(units=128,activation=\"relu\")(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "        \n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 20, 300)      4728900     input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_19 (SpatialDr (None, 20, 300)      0           embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 5, 4, 300)    0           spatial_dropout1d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 5, 4, 32)     9632        reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 5, 3, 32)     19232       reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 5, 2, 32)     28832       reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 5, 3, 32)     19232       reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 3, 32)     38432       reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 3, 3, 32)     57632       reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling2D) (None, 2, 2, 32)     0           conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_76 (MaxPooling2D) (None, 2, 1, 32)     0           conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_77 (MaxPooling2D) (None, 2, 1, 32)     0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling2D) (None, 2, 1, 32)     0           conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling2D) (None, 2, 1, 32)     0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling2D) (None, 1, 1, 32)     0           conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 128)          0           max_pooling2d_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 64)           0           max_pooling2d_76[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 64)           0           max_pooling2d_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 5, 1, 32)     0           max_pooling2d_72[0][0]           \n",
      "                                                                 max_pooling2d_73[0][0]           \n",
      "                                                                 max_pooling2d_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 256)          0           flatten_26[0][0]                 \n",
      "                                                                 flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 160)          0           concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 416)          0           concatenate_18[0][0]             \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 128)          53376       concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            129         dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,955,397\n",
      "Trainable params: 4,955,397\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 1600 samples\n",
      "Epoch 1/10\n",
      "14400/14400 [==============================] - 43s 3ms/sample - loss: 0.5080 - accuracy: 0.7628 - val_loss: 0.3490 - val_accuracy: 0.8938\n",
      "Epoch 2/10\n",
      "14400/14400 [==============================] - 43s 3ms/sample - loss: 0.3648 - accuracy: 0.8408 - val_loss: 0.2709 - val_accuracy: 0.8906\n",
      "Epoch 3/10\n",
      "14400/14400 [==============================] - 44s 3ms/sample - loss: 0.2645 - accuracy: 0.8887 - val_loss: 0.2736 - val_accuracy: 0.8875\n",
      "Epoch 4/10\n",
      "14400/14400 [==============================] - 46s 3ms/sample - loss: 0.1810 - accuracy: 0.9269 - val_loss: 0.2837 - val_accuracy: 0.8825\n",
      "Epoch 5/10\n",
      "14400/14400 [==============================] - 44s 3ms/sample - loss: 0.1270 - accuracy: 0.9503 - val_loss: 0.3090 - val_accuracy: 0.8775\n",
      "Epoch 6/10\n",
      "14400/14400 [==============================] - 43s 3ms/sample - loss: 0.0956 - accuracy: 0.9619 - val_loss: 0.3221 - val_accuracy: 0.8906\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "hist = model2.fit(train_cnn_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model2.predict(test_cnn_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i][0]<0.5):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "def check_metric(output_class_pred,original_ans):\n",
    "    rightly_predicted=0\n",
    "    TP=0\n",
    "    for i in range(len(y_test)):\n",
    "        if(original_ans[i]==output_class_pred[i]):\n",
    "            rightly_predicted+=1\n",
    "        \n",
    "        \n",
    "    print(\"Overall_acuracy:\",rightly_predicted/len(output_class_pred))\n",
    "    print('TP',TP)\n",
    "    accuracy=rightly_predicted/len(y_test)\n",
    "    print(classification_report(original_ans,output_class_pred))\n",
    "    print(confusion_matrix(original_ans,output_class_pred))\n",
    "    TN=confusion_matrix(original_ans,output_class_pred)[0][0]\n",
    "    TP=confusion_matrix(original_ans,output_class_pred)[1][1]\n",
    "    FP=confusion_matrix(original_ans,output_class_pred)[0][1]\n",
    "    FN=confusion_matrix(original_ans,output_class_pred)[1][0]\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recalll=TP/(FN+TP)\n",
    "    F1=2*precision*recalll/(precision+recalll)\n",
    "    sensiti=TP/(TP+FN)\n",
    "    specifici=TN/(TN+FP)\n",
    "    numerator=TP*TN - FP*FN\n",
    "    \n",
    "    denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
    "    MCc=numerator/denominator\n",
    "    G_mean1=np.sqrt(sensiti*precision)\n",
    "    G_mean2=np.sqrt(sensiti*specifici)\n",
    "    print('precision:' ,TP/(TP+FP))\n",
    "    print('recall:',TP/(FN+TP))\n",
    "    print(\"F1:\",F1)\n",
    "    print(\"Specificity:\",TN/(TN+FP))\n",
    "    print(\"Sensitivity \",TP/(TP+FN))\n",
    "    print('G-mean1:',np.sqrt(sensiti*precision))\n",
    "    print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
    "    print(\"MCC :\",MCc)\n",
    "    acc=[]\n",
    "    pre=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    specificity=[]\n",
    "    sensitivity=[]\n",
    "    GMean1=[]\n",
    "    Gmean2=[]\n",
    "    MCC=[]\n",
    "    tp=[]\n",
    "    fp=[]\n",
    "    fn=[]\n",
    "    tn=[]\n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    recall.append(recalll)\n",
    "    f1.append(F1)\n",
    "    specificity.append(specifici)\n",
    "    sensitivity.append(sensiti)\n",
    "    GMean1.append(G_mean1)\n",
    "    Gmean2.append(G_mean2)\n",
    "    MCC.append(MCc)\n",
    "    tp.append(TP)\n",
    "    fp.append(FP)\n",
    "    tn.append(TN)\n",
    "    fn.append(FN)\n",
    "    data={'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n",
    "    metric=pd.DataFrame(data)\n",
    "    return metric\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall_acuracy: 0.8214983713355049\n",
      "TP 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.58      1323\n",
      "           1       0.88      0.89      0.89      4817\n",
      "\n",
      "    accuracy                           0.82      6140\n",
      "   macro avg       0.74      0.73      0.73      6140\n",
      "weighted avg       0.82      0.82      0.82      6140\n",
      "\n",
      "[[ 747  576]\n",
      " [ 520 4297]]\n",
      "precision: 0.8817976605786989\n",
      "recall: 0.892048993149263\n",
      "F1: 0.8868937048503611\n",
      "Specificity: 0.564625850340136\n",
      "Sensitivity  0.892048993149263\n",
      "G-mean1: 0.8869085157335022\n",
      "G-mean2 0.7096998811483378\n",
      "MCC : 0.463968846698055\n"
     ]
    }
   ],
   "source": [
    "resi=check_metric(output_class_pred,original_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "resi.to_csv('results.csv', mode='w', index = False, header=resi.columns,columns=resi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('GOSSIP_mod2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_y=train_y[[0,1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "maxlen=20 #(MAX_SEQUENCE_LENGTH)\n",
    "max_sequence_length=20\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "\n",
    "#model3 yoon kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=2)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    #pool = MaxPooling1D(pool_size=2)(conv)\n",
    "\n",
    "    #if extra_conv==True:\n",
    "        #x = Dropout(0.01)(l_merge)  \n",
    "    #else:\n",
    "        # Original Yoon Kim model\n",
    "        #x = Dropout(0.001)(pool)\n",
    "    x = Flatten()(l_merge)\n",
    "    \n",
    "    x = Dropout(0.5)(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 20, 300)      4728900     input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 18, 100)      90100       embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 17, 100)      120100      embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 16, 100)      150100      embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 9, 100)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 8, 100)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 8, 100)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 25, 100)      0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 2500)         0           concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 2500)         0           flatten_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            5002        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,094,202\n",
      "Trainable params: 5,094,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                 True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 1600 samples\n",
      "Epoch 1/10\n",
      "14400/14400 [==============================] - 51s 4ms/sample - loss: 0.4901 - acc: 0.7756 - val_loss: 0.2386 - val_acc: 0.9275\n",
      "Epoch 2/10\n",
      "14400/14400 [==============================] - 47s 3ms/sample - loss: 0.3158 - acc: 0.8641 - val_loss: 0.2725 - val_acc: 0.8875\n",
      "Epoch 3/10\n",
      "14400/14400 [==============================] - 45s 3ms/sample - loss: 0.1980 - acc: 0.9197 - val_loss: 0.2276 - val_acc: 0.9106\n",
      "Epoch 4/10\n",
      "14400/14400 [==============================] - 44s 3ms/sample - loss: 0.1189 - acc: 0.9549 - val_loss: 0.2924 - val_acc: 0.8956\n",
      "Epoch 5/10\n",
      "14400/14400 [==============================] - 47s 3ms/sample - loss: 0.0819 - acc: 0.9712 - val_loss: 0.4134 - val_acc: 0.8731\n",
      "Epoch 6/10\n",
      "14400/14400 [==============================] - 43s 3ms/sample - loss: 0.0565 - acc: 0.9805 - val_loss: 0.4986 - val_acc: 0.8694\n",
      "Epoch 7/10\n",
      "14400/14400 [==============================] - 45s 3ms/sample - loss: 0.0528 - acc: 0.9840 - val_loss: 0.4113 - val_acc: 0.8875\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "hist = model3.fit(train_cnn_data, trains_y,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here forming th predicted array is little differnt (we calculate the argmax from pred and append into outpu accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model3.predict(test_cnn_data)\n",
    "y_test=pred\n",
    "y_test=y_test.tolist()\n",
    "output_class_pred=[]\n",
    "#output_class_pred=[]\n",
    "for i in range(len(y_test)):\n",
    "    m=max(y_test[i])\n",
    "    if(y_test[i].index(m)==0):\n",
    "        output_class_pred.append(0)\n",
    "    else:\n",
    "        output_class_pred.append(1)\n",
    "        \n",
    "        \n",
    "original_ans=data_test['tag']\n",
    "original_ans=original_ans.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as its a fake news classifier , so identifying a fake class will be a TP\n",
    "def check_metric(output_class_pred,original_ans):\n",
    "    rightly_predicted=0\n",
    "    TP=0\n",
    "    for i in range(len(y_test)):\n",
    "        if(original_ans[i]==output_class_pred[i]):\n",
    "            rightly_predicted+=1\n",
    "        \n",
    "        \n",
    "    print(\"Overall_acuracy:\",rightly_predicted/len(output_class_pred))\n",
    "    print('TP',TP)\n",
    "    accuracy=rightly_predicted/len(y_test)\n",
    "    print(classification_report(original_ans,output_class_pred))\n",
    "    print(confusion_matrix(original_ans,output_class_pred))\n",
    "    TN=confusion_matrix(original_ans,output_class_pred)[0][0]\n",
    "    TP=confusion_matrix(original_ans,output_class_pred)[1][1]\n",
    "    FP=confusion_matrix(original_ans,output_class_pred)[0][1]\n",
    "    FN=confusion_matrix(original_ans,output_class_pred)[1][0]\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recalll=TP/(FN+TP)\n",
    "    F1=2*precision*recalll/(precision+recalll)\n",
    "    sensiti=TP/(TP+FN)\n",
    "    specifici=TN/(TN+FP)\n",
    "    numerator=TP*TN - FP*FN\n",
    "    \n",
    "    denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
    "    MCc=numerator/denominator\n",
    "    G_mean1=np.sqrt(sensiti*precision)\n",
    "    G_mean2=np.sqrt(sensiti*specifici)\n",
    "    print('precision:' ,TP/(TP+FP))\n",
    "    print('recall:',TP/(FN+TP))\n",
    "    print(\"F1:\",F1)\n",
    "    print(\"Specificity:\",TN/(TN+FP))\n",
    "    print(\"Sensitivity \",TP/(TP+FN))\n",
    "    print('G-mean1:',np.sqrt(sensiti*precision))\n",
    "    print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
    "    print(\"MCC :\",MCc)\n",
    "    acc=[]\n",
    "    pre=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    specificity=[]\n",
    "    sensitivity=[]\n",
    "    GMean1=[]\n",
    "    Gmean2=[]\n",
    "    MCC=[]\n",
    "    tp=[]\n",
    "    fp=[]\n",
    "    fn=[]\n",
    "    tn=[]\n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    recall.append(recalll)\n",
    "    f1.append(F1)\n",
    "    specificity.append(specifici)\n",
    "    sensitivity.append(sensiti)\n",
    "    GMean1.append(G_mean1)\n",
    "    Gmean2.append(G_mean2)\n",
    "    MCC.append(MCc)\n",
    "    tp.append(TP)\n",
    "    fp.append(FP)\n",
    "    tn.append(TN)\n",
    "    fn.append(FN)\n",
    "    data={'accuracy_all':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n",
    "    metric=pd.DataFrame(data)\n",
    "    return metric\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall_acuracy: 0.8201954397394137\n",
      "TP 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.60      0.59      1323\n",
      "           1       0.89      0.88      0.88      4817\n",
      "\n",
      "    accuracy                           0.82      6140\n",
      "   macro avg       0.73      0.74      0.74      6140\n",
      "weighted avg       0.82      0.82      0.82      6140\n",
      "\n",
      "[[ 789  534]\n",
      " [ 570 4247]]\n",
      "precision: 0.8883078853796277\n",
      "recall: 0.8816690886443844\n",
      "F1: 0.8849760366743071\n",
      "Specificity: 0.5963718820861678\n",
      "Sensitivity  0.8816690886443844\n",
      "G-mean1: 0.8849822618212619\n",
      "G-mean2 0.7251225094920498\n",
      "MCC : 0.47343925526801317\n"
     ]
    }
   ],
   "source": [
    "resi=check_metric(output_class_pred,original_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "resi.to_csv('results.csv', mode='w', index = False, header=resi.columns,columns=resi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('GOSSIP_Yoonkim.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
